{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Title: Neural Networks Notes\n",
    "####Author: Tony Chang\n",
    "####Abstract: Notes from Michael Nielsen's online book regarding neural networks.\n",
    "http://neuralnetworksanddeeplearning.com/chap1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first lesson in this book is to understand how one recognizes handwritten digits. It is difficult to explicitly tell a computer that the number '9' is a circle with a tail that sometimes is curved, but sometimes is straight, and sometimes that circle is an oval, etc....So instead of trying do that, we provide a computer a dataset of many examples of '9', and let it learn what it looks like from many examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First term to understand is a **Perceptron**. A perceptron is essentially a function that takes in some number of inputs $x$, multiplies them by a weight $w$ and takes the sum (essentially a dot product) and then returns an output provided some logical set of rules and bias $b$ offset on the dot product. \n",
    "![perceptron](http://neuralnetworksanddeeplearning.com/images/tikz0.png)\n",
    "\n",
    "In the simplest case, we can have a perceptron return a 1 or 0 for the given dot product.\n",
    "\n",
    "$$\n",
    "\\text{output} = \\begin{cases} 0, \\text{if } w \\cdot x + b \\le 0 \\\\1, \n",
    "\\text{if } w \\cdot x + b > 0 \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's build a perceptron!\n",
    "def perceptron(x, w, b):\n",
    "    #input x and w arrays and b bias\n",
    "    #this perceptron builds a NAND gate\n",
    "    output = np.dot(x,w) + b\n",
    "    #logical test\n",
    "    if output > 0:\n",
    "        return(1)\n",
    "    elif output <= 0:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x11 = np.array([1,1])\n",
    "x01 = np.array([0,1])\n",
    "x10 = np.array([1,0])\n",
    "x00 = np.array([0,0])\n",
    "w = np.ones(2) * -2\n",
    "b = 3\n",
    "print(perceptron(x11, w, b))\n",
    "print(perceptron(x01, w, b))\n",
    "print(perceptron(x10, w, b))\n",
    "print(perceptron(x00, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can link perceptrons to get a sum of these inputs\n",
    "![NAND](http://neuralnetworksanddeeplearning.com/images/tikz4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NAND(x, w, b):\n",
    "    out1 = perceptron(x,w,b)\n",
    "    ox1 = np.array([x[0],out1])\n",
    "    ox2 = np.array([x[1],out1])\n",
    "    outx1 = perceptron(ox1,w,b)\n",
    "    outx2 = perceptron(ox2,w,b)\n",
    "    oxx = np.array([outx1,outx2])\n",
    "    sumx = perceptron(oxx, w, b)\n",
    "    carrybit = perceptron(np.array([out1,out1]), w,b) # this is the 2*1 bit (i.e. 2)\n",
    "    return(sumx, carrybit)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 00 and 00 is 00.\n",
      "The sum of 00 and 01 is 01.\n",
      "The sum of 01 and 00 is 01.\n",
      "The sum of 01 and 01 is 10.\n"
     ]
    }
   ],
   "source": [
    "w = np.ones(2) * -2\n",
    "b = 3\n",
    "\n",
    "for x1 in range(2):\n",
    "    for x2 in range(2):\n",
    "        x = np.array([x1,x2])\n",
    "        nandx = NAND(x,w,b)\n",
    "        print(\"The sum of 0%d and 0%d is %d%d.\"%(x1,x2,nandx[1],nandx[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks great, we a neural network used as a NAND gate for computation. So perceptrons can be used for computation in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to introduce the **Sigmoid neuron**. In the previous example, the perceptron could only provide binary output 0 or 1. But, if we allow the output to be continuous between 0 and 1, then this would allow us to construct a perceptron that can slightly change the output by slight changes in either the weight $w$ or bias $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remember from statistics, if we want a function to run from 0 to 1 in a continuous manner, we can just use a logistic function:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "\n",
    "This is also known as the **sigmoid function** in machine learning or neural networks. But, this is a specific case, it can be any function, also called an _activation function_ $f(\\cdot)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f2564824438>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEZCAYAAACHCd7XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nc36Pxt8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f256c051f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5,5,1000)\n",
    "def sigmoid(z):\n",
    "    return(1/(1+np.e**(-z)))\n",
    "sig = sigmoid(z)\n",
    "plt.plot(z, sig)\n",
    "plt.xlabel('Z')\n",
    "plt.title('sigmoid function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can apply the sigmoid activation function to our perceptron in the form:\n",
    "$$\n",
    "\\sigma(w \\cdot x + b) = \\text(output)\n",
    "$$\n",
    "Noting that small changes in $w$ and $b$ can create small changes in the output:\n",
    "$$\n",
    "\\Delta\\text{output} \\approx \\sum_j \\frac{\\partial\\text{output}}{\\partial w_j} \\Delta w_j + \\frac{\\partial\\text{output}}{\\partial b}\\Delta b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Exercise 1: Sigmoid neurons simulating perceptrons, part I - Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, $c>0$. Show that the behaviour of the network doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 00 and 00 is 00.\n",
      "The sum of 00 and 01 is 01.\n",
      "The sum of 01 and 00 is 01.\n",
      "The sum of 01 and 01 is 10.\n",
      "c = 1.723413\n"
     ]
    }
   ],
   "source": [
    "w = np.ones(2) * -2\n",
    "b = 3\n",
    "c = np.random.random(1) * (np.random.random(1)*10)\n",
    "w = w * c\n",
    "b = b * c\n",
    "for x1 in range(2):\n",
    "    for x2 in range(2):\n",
    "        x = np.array([x1,x2])\n",
    "        nandx = NAND(x,w,b)\n",
    "        print(\"The sum of 0%d and 0%d is %d%d.\"%(x1,x2,nandx[1],nandx[0]))\n",
    "print(\"c = %f\"%c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behaviour of the NAND perceptron does not change with constant $c > 0$ multiplying $w$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Exercise 2: Sigmoid neurons simulating perceptrons, part II - Suppose we have the same setup as the last problem - a network of perceptrons. Suppose also that the overall input to the network of perceptrons has been chosen. We won't need the actual input value, we just need the input to have been fixed. Suppose the weights and biases are such that $w⋅x+b≠0$ for the input x to any particular perceptron in the network. Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant c>0. Show that in the limit as $c→∞$ the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when $w⋅x+b=0$ for one of the perceptrons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $c \\rightarrow \\infty$ then $z \\rightarrow -\\infty$ so the value sigmoid neuron approaches 0. If $w \\cdot x + b =0$ then our sigmoid will equal $\\frac{1}{1+1}$ which equals $\\frac{1}{2}$. This is not an output of our perceptron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's build a this perceptron!\n",
    "#in this case w dot x + b can not equal 0, so x is some fixed value. Let's use x = np.array([1,1])\n",
    "def sigmoid(x, w, b):\n",
    "    #input x and w arrays and b bias\n",
    "    #this perceptron builds a NAND gate\n",
    "    output = 1/(1+ np.e**(-1*(np.dot(x,w) + b)))\n",
    "    #logical test\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   1.5  0.   0.  -0.5]\n",
      " [ 1.5  2.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [-0.5  0.   0.   0.  -2. ]]\n",
      "[[ 28.  27.   0.   0.  31.]\n",
      " [ 27.  26.   0.   0.  30.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [ 31.  30.   0.   0.  34.]]\n",
      "[[ 1.  1.  0.  0.  1.]\n",
      " [ 1.  1.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0.  1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f2564743898>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEACAYAAADbQ0FgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f256c04d390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = 5\n",
    "w = np.ones(2) *-2\n",
    "b = 30\n",
    "o = np.zeros((j,j))\n",
    "zs = np.zeros((j,j))\n",
    "xs = np.zeros((j,j))\n",
    "r = np.linspace(-1,1,j)\n",
    "\n",
    "for x1 in r:\n",
    "    for x2 in r:\n",
    "        x = np.array([x1,x2])\n",
    "        xs[x1,x2] = np.sum(x)\n",
    "        zs[x1,x2] = np.dot(x,w) + b\n",
    "        o[x1,x2]=sigmoid(x,w,b)\n",
    "print(xs)\n",
    "print(zs)\n",
    "print(o)\n",
    "plt.set_cmap('viridis')\n",
    "plt.imshow(o, origin = 'lower', extent = [o.min(), o.max(), o.min(), o.max()]); plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about this exercise more...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Introducing the cost function\n",
    "So the first cost function introduced is:\n",
    "$$\n",
    "C(w,b) \\equiv \\frac{1}{2n} \\sum_x \\parallel y(x) - a \\parallel^2\n",
    "$$\n",
    "Which is just the mean squared error. Where $a$ is a vector of length $n$ (total number of training values) the correct classifications for the numbers (0,9), represented in binary (1 is the class and 0 is not the class), and $y(x)$ is the output from the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We would like to minimize this cost function. So let's consider what a cost function could look like. Imagining a case for 2 variables $v_1$ and $v_2$ we can visualize a cost function as:\n",
    "![cost_func](http://neuralnetworksanddeeplearning.com/images/valley.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this cost function we can find what values of $v$, the variables (or the weights and the bias in our perceptron model), that we should use by taking the gradient of $C$ (the partial derivatives in terms of each input variable) times some negative learning rate $\\eta$. We use this value to change our next input weight and bias for the each training data set. It should be noted that this would mean that we adjust the weight and bias with each individual training data input $n$ and take the mean of it which could take a great deal of time. One solution is to take a subset of the data of size $m$ randomly and calculate the gradient of the cost function on those only, this known as *stochastic gradient descent*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Coding the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the neural network object\n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#instantiate a network with 2 neurons in the first layer (two inputs), 3 in the second (3x2 weights, and 3x1 biases), \n",
    "#and one in the third (1x3 weights and 1 bias)\n",
    "net = Network([2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.31095052, -0.19525162],\n",
       "        [-0.02360752,  2.65249676],\n",
       "        [ 0.32104061,  0.11977198]]),\n",
       " array([[-1.09412191,  0.3490986 , -2.06342049]])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can see this if we inspect weights and bias. They are drawn from a Gaussian distribution as a start\n",
    "net.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define sigmoid function again\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3\n",
      "2 4\n",
      "3 5\n",
      "4 6\n"
     ]
    }
   ],
   "source": [
    "#reminder of what \"zip\" does\n",
    "a = np.arange(1,5)\n",
    "b = np.arange(3,8)\n",
    "for i,j in zip(a,b):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "#define the neural network object\n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    #now define stochastic gradient descent\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The \"training_data\" is a list of tuples\n",
    "        \"(x, y)\" representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If \"test_data\" is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: \n",
    "            n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data) #shuffle up the training data\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size] \n",
    "                for k in range(0, n, mini_batch_size)] #make a mini batch of mini_batch_size\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch %i %i / %i\"%(j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch %i complete\"%j)\n",
    "\n",
    "    #define the update_mini_batch function as well\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases] #creates place holder for the biases\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights] #creates place holder for the weights\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw \n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb \n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    #functions for calculating the cost function and gradient of cost function\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return(a)\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize our network for the MNIST data with 30 hidden layers and 10 final layers.\n",
    "net = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 9135 / 10000\n",
      "Epoch 1 9247 / 10000\n",
      "Epoch 2 9319 / 10000\n",
      "Epoch 3 9346 / 10000\n",
      "Epoch 4 9403 / 10000\n",
      "Epoch 5 9410 / 10000\n",
      "Epoch 6 9436 / 10000\n",
      "Epoch 7 9431 / 10000\n",
      "Epoch 8 9444 / 10000\n",
      "Epoch 9 9431 / 10000\n",
      "Epoch 10 9466 / 10000\n",
      "Epoch 11 9491 / 10000\n",
      "Epoch 12 9459 / 10000\n",
      "Epoch 13 9463 / 10000\n",
      "Epoch 14 9453 / 10000\n",
      "Epoch 15 9462 / 10000\n",
      "Epoch 16 9495 / 10000\n",
      "Epoch 17 9466 / 10000\n",
      "Epoch 18 9451 / 10000\n",
      "Epoch 19 9478 / 10000\n",
      "Epoch 20 9486 / 10000\n",
      "Epoch 21 9478 / 10000\n",
      "Epoch 22 9475 / 10000\n",
      "Epoch 23 9503 / 10000\n",
      "Epoch 24 9489 / 10000\n",
      "Epoch 25 9513 / 10000\n",
      "Epoch 26 9445 / 10000\n",
      "Epoch 27 9498 / 10000\n",
      "Epoch 28 9489 / 10000\n",
      "Epoch 29 9499 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data) #run the stochastic gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
